{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis of CellFishing.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Table of contents](#Table-of-contents)\n",
    "- [Cell-type distributions](#Cell-type-distributions)\n",
    "- [Randomized SVD](#Randomized-SVD)\n",
    "- [Similarity estimation via hashing](#Similarity-estimation-via-hashing)\n",
    "- [Visualization of expression profiles](#Visualization-of-expression-profiles)\n",
    "- [Self-mapping experiments](#Self-mapping-experiments)\n",
    "- [Feature selection](#Feature-selection)\n",
    "- [Cluster-specific scores](#Cluster-specific-scores)\n",
    "- [Similarities from nearest neighbors](#Similarities-from-nearest-neighbors)\n",
    "- [Detecting DEGs](#Detecting-DEGs)\n",
    "- [Mapping across batches](#Mapping-across-batches)\n",
    "- [Mapping across species](#Mapping-across-species)\n",
    "- [Mapping across protocols](#Mapping-across-protocols)\n",
    "- [Benchmarks of saving and loading](#Benachmarks-of-saving-and-loading)\n",
    "- [Scalability](#Scalability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import textwrap\n",
    "import toml\n",
    "import numpy\n",
    "import pandas\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.pyplot\n",
    "import matplotlib_venn\n",
    "import seaborn\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkgname = \"CellFishing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = \"pdf\"\n",
    "dpi = 200\n",
    "seaborn.set_style(\"ticks\", {\"image.cmap\": \"viridis\"})\n",
    "\n",
    "def savefig(name):\n",
    "    matplotlib.pyplot.savefig(os.path.join(\"figures\", f\"{name}.{ext}\"), dpi=\"figure\", bbox_inches=\"tight\")\n",
    "    \n",
    "!mkdir -p figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell-type distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"Baron2016-human\", \"Shekhar2016\", \"Plass2018\", \"TabulaMuris-chromium\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cell-type annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = dict()\n",
    "for dataset in datasets:\n",
    "    clusters[dataset] = pandas.read_table(f\"data/{dataset}.cluster.tsv\", index_col=\"cell\")[\"cluster\"]\n",
    "clusters[\"Baron2016-mouse\"] = pandas.read_table(\"data/Baron2016-mouse.cluster.tsv\", index_col=\"cell\")[\"cluster\"]\n",
    "clusters[\"Baron2016-human\"][clusters[\"Baron2016-human\"] == \"t_cell\"] = \"T_cell\"  # fix the name to match that of mouse\n",
    "clusters[\"TabulaMuris-smart\"] = pandas.read_table(\"data/TabulaMuris-smart.cluster.tsv\", index_col=\"cell\")[\"cluster\"]\n",
    "clusters[\"1M_neurons\"] = pandas.read_table(\"data/1M_neurons.cluster.tsv\", index_col=\"cell\")[\"cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:20s}  {:>9}  {:>9s}\".format(\"dataset\", \"#cells\", \"#clusters\"))\n",
    "print(\"------------------------------------------\")\n",
    "for dataset, cluster in sorted(clusters.items()):\n",
    "    print(\"{:20s}  {:>9,d}  {:>9,d}\".format(dataset, len(cluster), len(cluster.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster (cell type) distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, cluster in clusters.items():\n",
    "    print(dataset)\n",
    "    fig, ax = subplots(dpi=dpi, figsize=(8, 10))\n",
    "    counts = cluster.value_counts()\n",
    "    seaborn.countplot(\n",
    "        y=\"cluster\",\n",
    "        data=pandas.DataFrame(cluster),\n",
    "        order=counts.index.values,\n",
    "        color=\"C0\",\n",
    "        ax=ax)\n",
    "    fontsize = 6.5\n",
    "    for i, c in enumerate(counts.index.values):\n",
    "        ax.text(counts[c], i, \" {:,} ({:.2f}%)\".format(counts[c], counts[c] / len(cluster) * 100),\n",
    "                verticalalignment=\"center\",\n",
    "                fontsize=fontsize)\n",
    "    ax.tick_params(labelsize=fontsize)\n",
    "    ax.set_xlabel(\"cluster size\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    savefig(f\"{dataset}-clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load benchmark results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_singular_values(data):\n",
    "    data = pandas.concat([\n",
    "        pandas.DataFrame({\"k\": numpy.arange(1, r[\"n-dims\"]+1), **dict(r)}) for ix, r in data.iterrows()], ignore_index=True)\n",
    "    return data.rename(columns={\"singular-values\": \"singular-value\"})\n",
    "\n",
    "svd = pandas.DataFrame()\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    out = toml.load(f\"results/{dataset}.svd.toml\")\n",
    "    print(out[\"date-time\"])\n",
    "    print(out[\"version-info\"])\n",
    "    expanded = expand_singular_values(pandas.DataFrame(out[\"experiment\"]))\n",
    "    expanded[\"dataset\"] = dataset\n",
    "    svd = svd.append(expanded, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.groupby([\"dataset\"]).first()[[\"n-rows\", \"n-columns\"]].loc[datasets,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd[svd[\"k\"]==1].groupby([\"dataset\", \"algorithm\"]).describe()[\"time-svd\"].loc[datasets,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the elapsed time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(dpi=dpi, figsize=(8, 4))\n",
    "seaborn.swarmplot(x=\"dataset\", y=\"time-svd\", hue=\"algorithm\", data=svd[svd[\"k\"]==1], dodge=True, size=3)\n",
    "ax.set_ylabel(\"elapsed time [s]\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylim(0, None)\n",
    "ax.locator_params(nbins=5)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(f\"svd-benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(4, sharex=True, dpi=dpi, figsize=(8, 4))\n",
    "for i, dataset in enumerate(datasets):\n",
    "    ax = axes[i]\n",
    "    seaborn.pointplot(x=\"k\", y=\"singular-value\", hue=\"algorithm\", data=svd[svd[\"dataset\"]==dataset], scale=0.5, ci=\"sd\", ax=ax)\n",
    "    if i == 3:\n",
    "        ax.set_xlabel(\"rank\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"\")\n",
    "    ax.set_ylim(0, None)\n",
    "    for k, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "        if k % 2 == 1:\n",
    "            label.set_visible(False)\n",
    "    ax.legend_.remove()\n",
    "    ax.locator_params(axis=\"y\", nbins=1)\n",
    "    ax.set_ylabel(\"$\\sigma$\")\n",
    "    ax.set_title(dataset)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(f\"svd-singular-values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the relative errors of the singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svals = svd[svd[\"algorithm\"]==\"full\"][\"singular-value\"].values\n",
    "relerror = svd[svd[\"algorithm\"]==\"rand\"]\\\n",
    "    .assign(**{\"relative-error\": lambda x: numpy.abs(1 - x[\"singular-value\"] / svals)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the relative errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(4, sharex=True, dpi=dpi, figsize=(8, 4))\n",
    "for i, dataset in enumerate(datasets):\n",
    "    ax = axes[i]\n",
    "    seaborn.pointplot(x=\"k\", y=\"relative-error\", data=relerror[relerror[\"dataset\"]==dataset], scale=0.5, errwidth=1.0, ci=\"sd\", ax=ax)\n",
    "    if i == 3:\n",
    "        ax.set_xlabel(\"rank\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"\")\n",
    "    ax.set_ylim(0, None)\n",
    "    for k, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "        if k % 2 == 1:\n",
    "            label.set_visible(False)\n",
    "    ax.set_ylabel(\"rel. error\")\n",
    "    ax.set_title(dataset)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(f\"svd-relative-errors\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity estimation via hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:20s}  {:>6s}  {:>8s}  {:>7s}  {:>6s}\".format(\"dataset\", \"n-bits\", \"superbit\", \"mean\", \"std\"))\n",
    "for dataset in datasets:\n",
    "    fig, axes = subplots(4, dpi=dpi, figsize=(8, 5), sharex=True)\n",
    "    for i, n_bits in enumerate([64, 128, 256, 512]):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"n-bits = {n_bits}\")\n",
    "        ax.set_xlim(-0.75, +0.75)\n",
    "        for j, superbit in enumerate([1, 50]):\n",
    "            for r in range(1, 6):\n",
    "                cosdist = numpy.loadtxt(f\"results/{dataset}.estimator/n_bits-{n_bits}.superbit-{superbit}.{r}.cos.tsv.gz\")\n",
    "                angle = numpy.arccos(-cosdist+1)\n",
    "                hamdist = numpy.loadtxt(f\"results/{dataset}.estimator/n_bits-{n_bits}.superbit-{superbit}.{r}.ham.tsv.gz\")\n",
    "                approx = hamdist * math.pi / n_bits\n",
    "                values = (approx - angle).ravel()\n",
    "                print(\"{:20s}  {:>6d}  {:>8d}  {:>+.4f}  {:>.4f}\".format(dataset, n_bits, superbit, numpy.mean(values), numpy.std(values)))\n",
    "                kde = scipy.stats.gaussian_kde(values)\n",
    "                x = numpy.linspace(values.min(), values.max(), num=200)\n",
    "                ax.plot(x, kde(x), lw=1, color=f\"C{j}\")\n",
    "    axes[-1].set_xlabel(\"estimation error [rad]\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout()\n",
    "    savefig(f\"{dataset}-estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_positives(hamdist, cosdist, k):\n",
    "    hamord = numpy.argsort(hamdist, axis=1)[:,0:k+1]\n",
    "    cosord = numpy.argsort(cosdist, axis=1)[:,1]\n",
    "    n = 0\n",
    "    for i in range(hamord.shape[0]):\n",
    "        n += numpy.sum(numpy.isin(cosord[i], hamord[i]))\n",
    "    return n\n",
    "\n",
    "n_cells = 6\n",
    "x_max = 0.3  # upper limit of cosine similarity\n",
    "print(\"{:20s}  {:>6s}  {:>2s}  {:>11s}\".format(\"dataset\", \"n-bits\", \"k\", \"n-positives\"))\n",
    "for dataset in datasets:\n",
    "    fig, axes = subplots(4, n_cells, dpi=dpi, figsize=(8, 5), sharex=True, sharey=True)\n",
    "    for i, n_bits in enumerate([64, 128, 256, 512]):\n",
    "        cosdist = numpy.loadtxt(f\"results/{dataset}.estimator/n_bits-{n_bits}.superbit-50.1.cos.tsv.gz\")\n",
    "        hamdist = numpy.loadtxt(f\"results/{dataset}.estimator/n_bits-{n_bits}.superbit-50.1.ham.tsv.gz\")\n",
    "        for k in [10, 30, 50]:\n",
    "            print(\"{:20s}  {:>6d}  {:>2d}  {:>11d}\".format(dataset, n_bits, k, n_positives(hamdist, cosdist, k)))\n",
    "        axes[i,0].set_ylabel(f\"n-bits = {n_bits}\\n\\nHamming dist.\")\n",
    "        for j in range(n_cells):\n",
    "            ax = axes[i,j]\n",
    "            x = cosdist[j,:]\n",
    "            y = hamdist[j,:] / n_bits\n",
    "            ax.scatter(x[x<x_max], y[x<x_max], s=0.3, lw=0, c=\"C1\")\n",
    "            ax.set_xlim(0, x_max)\n",
    "            ax.set_ylim(0, None)\n",
    "    for j in range(n_cells):    \n",
    "        axes[3,j].set_xlabel(\"cosine dist.\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    savefig(f\"{dataset}-correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of expression profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell types and query/database plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readlines(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "cluster = clusters[\"Shekhar2016\"]\n",
    "tab20 = matplotlib.cm.get_cmap(\"tab20\")\n",
    "for query in [\"1\", \"2\"]:\n",
    "    print(f\"query batch: {query}\")\n",
    "    log = toml.load(f\"results/Shekhar2016.hash.batch-{query}.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"version-info\"])\n",
    "    outdir = f\"results/Shekhar2016.hash.batch-{query}\"\n",
    "    for metric in [\"euclidean\", \"cosine\"]:\n",
    "        fig, axes = subplots(2, 2, dpi=dpi, figsize=(8, 7))\n",
    "        for ax in axes.ravel():\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "        for i, project in enumerate([\"false\", \"true\"]):\n",
    "            cells_query = readlines(f\"{outdir}/cells.query.project-{project}.txt\")\n",
    "            X_query = numpy.loadtxt(f\"{outdir}/X.query.project-{project}.tsv\").T\n",
    "            cells_database = readlines(f\"{outdir}/cells.database.project-{project}.txt\")\n",
    "            X_database = numpy.loadtxt(f\"{outdir}/X.database.project-{project}.tsv\").T\n",
    "            X = numpy.vstack([X_query, X_database])\n",
    "            X_umap = umap.UMAP(metric=metric, random_state=1234).fit_transform(X)\n",
    "            shuf = numpy.arange(X_umap.shape[0])\n",
    "            numpy.random.RandomState(1234).shuffle(shuf)\n",
    "            labels = cluster[cells_query + cells_database]\n",
    "            label2color = {label: tab20(k) for k, label in enumerate(numpy.sort(labels.unique()))}\n",
    "            color = numpy.array([label2color[label] for label in labels])\n",
    "            axes[i,0].scatter(X_umap[shuf,0], X_umap[shuf,1], s=0.5, lw=0, c=color[shuf])\n",
    "            color = numpy.array([\"C1\"] * len(cells_query) + [\"C0\"] * len(cells_database))\n",
    "            axes[i,1].scatter(X_umap[shuf,0], X_umap[shuf,1], s=0.5, lw=0, c=color[shuf])\n",
    "        axes[0,0].set_title(\"cell type\")\n",
    "        axes[0,1].set_title(\"query/database\")\n",
    "        axes[0,0].set_ylabel(\"projected = false\")\n",
    "        axes[1,0].set_ylabel(\"projected = true\")\n",
    "        #fig.suptitle(f\"query = {query}\", va=\"bottom\")\n",
    "        seaborn.despine(fig=fig)\n",
    "        fig.tight_layout(pad=0.5)\n",
    "        savefig(f\"Shekhar2016-hash-query-{query}-metric-{metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy\n",
    "cimport numpy\n",
    "cimport cython\n",
    "\n",
    "cdef extern int __builtin_popcount(unsigned int)\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cdef distance_matrix(numpy.ndarray[numpy.uint8_t,ndim=1] rows,\n",
    "                     numpy.ndarray[numpy.uint8_t,ndim=1] cols,\n",
    "                     int nbytes,\n",
    "                     numpy.ndarray[numpy.uint16_t,ndim=2] output):\n",
    "    cdef int i, j, k\n",
    "    cdef int d\n",
    "    cdef int m = rows.shape[0] // nbytes\n",
    "    cdef int n = cols.shape[0] // nbytes\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            d = 0\n",
    "            for k in range(nbytes):\n",
    "                d += __builtin_popcount(rows[i*nbytes+k] ^ cols[j*nbytes+k])\n",
    "            output[i,j] = d\n",
    "\n",
    "            \n",
    "def hamming_distance_matrix(rows, cols, nbytes):\n",
    "    assert rows.shape[0] % nbytes == 0\n",
    "    assert cols.shape[0] % nbytes == 0\n",
    "    m = rows.shape[0] // nbytes\n",
    "    n = cols.shape[0] // nbytes\n",
    "    output = numpy.zeros((m, n), dtype=numpy.uint16)\n",
    "    distance_matrix(rows, cols, nbytes, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbytes = 64 // 8\n",
    "cluster = clusters[\"Shekhar2016\"]\n",
    "tab20 = matplotlib.cm.get_cmap(\"tab20\")\n",
    "n = len(cluster)\n",
    "shuf = numpy.arange(n)\n",
    "numpy.random.RandomState(1234).shuffle(shuf)\n",
    "for query in [\"1\", \"2\"]:\n",
    "    fig, axes = subplots(2, 2, dpi=dpi, figsize=(8, 8))\n",
    "    axes = [axes[0,0], axes[0,1], axes[1,0], axes[1,1]]\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "    outdir = f\"results/Shekhar2016.hash.batch-{query}\"\n",
    "    cells_query = readlines(f\"{outdir}/cells.query.project-true.txt\")\n",
    "    X_query = numpy.loadtxt(f\"{outdir}/X.query.project-true.tsv\").T\n",
    "    cells_database = readlines(f\"{outdir}/cells.database.project-true.txt\")\n",
    "    X_database = numpy.loadtxt(f\"{outdir}/X.database.project-true.tsv\").T\n",
    "    X = numpy.vstack([X_query, X_database])\n",
    "    X_umap = umap.UMAP(random_state=1234, metric=\"cosine\").fit_transform(X)\n",
    "    # plot\n",
    "    labels = cluster[cells_query + cells_database]\n",
    "    label2color = {label: tab20(k) for k, label in enumerate(numpy.sort(labels.unique()))}\n",
    "    color = numpy.array([label2color[label] for label in labels])\n",
    "    ax = axes.pop(0)\n",
    "    ax.scatter(X_umap[shuf,0], X_umap[shuf,1], s=0.5, lw=0, c=color[shuf])\n",
    "    ax.set_title(\"cosine distance (original)\")\n",
    "    # hashed profiles\n",
    "    D = numpy.zeros((n, n), dtype=numpy.uint16)\n",
    "    for l in range(4):\n",
    "        # compute distance matrix\n",
    "        Z_query = numpy.fromfile(f\"{outdir}/Z.query.project-true.{l+1}.bin\", dtype=numpy.uint8)\n",
    "        Z_database = numpy.fromfile(f\"{outdir}/Z.database.project-true.{l+1}.bin\", dtype=numpy.uint8)\n",
    "        Z = numpy.hstack([Z_query, Z_database])\n",
    "        D += hamming_distance_matrix(Z, Z, nbytes)\n",
    "        nbits = nbytes * (l+1) * 8\n",
    "        if nbits in (64, 128, 256):\n",
    "            D_umap = umap.UMAP(random_state=1234, metric=\"precomputed\").fit_transform(D)         \n",
    "            ax = axes.pop(0)\n",
    "            ax.scatter(D_umap[shuf,0], D_umap[shuf,1], s=0.5, lw=0, c=color[shuf])\n",
    "            ax.set_title(f\"Hamming distance (n-bits = {nbits})\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    savefig(f\"Shekhar2016-hash-query-{query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-mapping experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knn(filename):\n",
    "    return pandas.read_table(filename, index_col=\"cell\")\n",
    "\n",
    "# Compute the consistency, Cohen's kappa, and the adjusted Rand scores.\n",
    "def cluster_metrics(filename, cluster, k=1, cluster_nn=None):\n",
    "    if cluster_nn is None:\n",
    "        cluster_nn = cluster\n",
    "    knn = load_knn(filename)\n",
    "    cell = cluster[knn.index].values\n",
    "    nn = cluster_nn[knn[f\"N{k}\"]].values\n",
    "    return dict(\n",
    "        consistency=sklearn.metrics.accuracy_score(cell, nn),\n",
    "        cohen_kappa=sklearn.metrics.cohen_kappa_score(cell, nn),\n",
    "        adjusted_rand=sklearn.metrics.adjusted_rand_score(cell, nn),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = pandas.DataFrame()\n",
    "knn_cv_scmap = pandas.DataFrame()\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    # CellFishing\n",
    "    log = toml.load(f\"results/{dataset}.knn-cv.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"version-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"dataset\"] = dataset\n",
    "    exp = pandas.concat(\n",
    "        [exp, pandas.DataFrame([cluster_metrics(x, clusters[dataset], k=1) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_cv = knn_cv.append(exp, ignore_index=True)\n",
    "    # scmap\n",
    "    log = toml.load(f\"results/{dataset}.knn-cv.scmap.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"session-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"dataset\"] = dataset\n",
    "    exp = pandas.concat(\n",
    "        [exp, pandas.DataFrame([cluster_metrics(x, clusters[dataset], k=1) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_cv_scmap = knn_cv_scmap.append(exp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = knn_cv[[\"dataset\", \"n-bits\", \"n-lshashes\", \"transformer\", \"filename\", \"consistency\", \"cohen_kappa\", \"adjusted_rand\", \"index-time\", \"query-time\"]]\n",
    "knn_cv_scmap = knn_cv_scmap[[\"dataset\", \"n-features\", \"n-centroids-factor\", \"filename\", \"consistency\", \"cohen_kappa\", \"adjusted_rand\", \"index-time\", \"query-time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.options.display.max_columns = 99\n",
    "knn_cv.groupby([\"dataset\", \"n-bits\", \"n-lshashes\", \"transformer\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_scmap.groupby([\"dataset\", \"n-features\", \"n-centroids-factor\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the elapsed time with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_elapsed = knn_cv[(knn_cv[\"n-bits\"]==128)&(knn_cv[\"n-lshashes\"]==4)&(knn_cv[\"transformer\"]==\"log1p\")][[\"dataset\", \"index-time\", \"query-time\"]].groupby(\"dataset\").median()\n",
    "knn_cv_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_scmap_elapsed = knn_cv_scmap[(knn_cv_scmap[\"n-features\"]==500)&(knn_cv_scmap[\"n-centroids-factor\"]==1)][[\"dataset\", \"index-time\", \"query-time\"]].groupby(\"dataset\").median()\n",
    "knn_cv_scmap_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_scmap_elapsed / knn_cv_elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot consistency scores, Cohen's kappa coefficients, index times, and query times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_labels = {\n",
    "    \"consistency\": \"consistency\",\n",
    "    \"cohen_kappa\": \"Cohen's kappa\",\n",
    "    \"index-time\": \"index time [s]\",\n",
    "    \"query-time\": \"query time [s]\"}\n",
    "transformer = \"log1p\"\n",
    "for metric in [\"consistency\", \"cohen_kappa\", \"index-time\", \"query-time\"]:\n",
    "    fig, axes = subplots(len(datasets), 2, dpi=dpi, figsize=(8, 7))\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        if i > 0:\n",
    "            axes[i,0].get_shared_x_axes().join(axes[0,0], axes[i,0])\n",
    "            axes[i,1].get_shared_x_axes().join(axes[0,1], axes[i,1])\n",
    "        axes[i,0].get_shared_y_axes().join(axes[i,0], axes[i,1])\n",
    "        data = knn_cv[(knn_cv[\"dataset\"]==dataset)&(knn_cv[\"transformer\"]==transformer)]\n",
    "        data_scmap = knn_cv_scmap[knn_cv_scmap[\"dataset\"]==dataset]\n",
    "        ymin = min(data[metric].min(), data_scmap[metric].min())\n",
    "        ymax = max(data[metric].max(), data_scmap[metric].max())\n",
    "        ydiff = ymax - ymin\n",
    "        markerscale = 0.5\n",
    "        labelspacing = 0.2\n",
    "        if metric.endswith(\"-time\"):\n",
    "            seaborn.swarmplot(x=\"n-bits\", hue=\"n-lshashes\", y=metric, data=data, ax=axes[i,0], size=3, dodge=True)\n",
    "            seaborn.swarmplot(x=\"n-centroids-factor\", hue=\"n-features\", y=metric, data=data_scmap, ax=axes[i,1], size=3, dodge=True)\n",
    "            axes[i,0].set_ylim(0, None)\n",
    "            axes[i,0].legend(title=\"n-lshashes\", fontsize=\"small\", markerscale=markerscale)\n",
    "            axes[i,1].legend(title=\"n-features\", fontsize=\"small\", markerscale=markerscale, loc=\"upper left\", labelspacing=labelspacing)\n",
    "        else:\n",
    "            axes[i,0].set_ylim(ymin - ydiff * 0.02, ymax + ydiff * 0.02)  # need to set ylim before plotting\n",
    "            seaborn.swarmplot(x=\"n-bits\", hue=\"n-lshashes\", y=metric, data=data, ax=axes[i,0], size=3, dodge=True)\n",
    "            seaborn.swarmplot(x=\"n-centroids-factor\", hue=\"n-features\", y=metric, data=data_scmap, ax=axes[i,1], size=3, dodge=True)\n",
    "            axes[i,0].legend(title=\"n-lshashes\", fontsize=\"small\", markerscale=markerscale, loc=\"lower right\")\n",
    "            axes[i,1].legend(title=\"n-features\", fontsize=\"small\", markerscale=markerscale, labelspacing=labelspacing)\n",
    "        if i > 0:\n",
    "            axes[i,0].legend_.remove()\n",
    "            axes[i,1].legend_.remove()\n",
    "        axes[i,0].set_ylabel(dataset + \"\\n\\n\" + metric_labels[metric])\n",
    "        axes[i,1].set_ylabel(\"\")\n",
    "        axes[i,1].set_yticklabels([])\n",
    "        for j in range(2):\n",
    "            axes[i,j].locator_params(axis=\"y\", nbins=4)\n",
    "            axes[i,j].grid(axis=\"y\", alpha=0.3)\n",
    "        if i < len(datasets) - 1:\n",
    "            for j in range(2):\n",
    "                axes[i,j].set_xlabel(\"\")\n",
    "                axes[i,j].set_xticklabels([])\n",
    "    axes[0,0].set_title(\"CellFishing\")\n",
    "    axes[0,1].set_title(\"scmap-cell\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout(h_pad=0.2)\n",
    "    savefig(f\"selfmapping-{metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the adjusted Rand scores from SC3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for dataset in datasets:\n",
    "    # The TOML files were broken due to the noisy outputs from SC3.\n",
    "    cluster = clusters[dataset]\n",
    "    k = len(cluster.unique())\n",
    "    for r in range(5):\n",
    "        sc3 = pandas.read_table(f\"results/{dataset}.clustering-sc3/k-{k}.{r+1}.tsv.gz\", index_col=\"cell\")[\"cluster\"]\n",
    "        assert sc3.index.equals(cluster.index)\n",
    "        score = sklearn.metrics.adjusted_rand_score(cluster, sc3)\n",
    "        data.append((dataset, score))\n",
    "sc3 = pandas.DataFrame(data, columns=[\"dataset\", \"adjusted_rand\"])\n",
    "sc3.groupby(\"dataset\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare default features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readlines(filename):\n",
    "    with open(filename) as file:\n",
    "        return [line.rstrip() for line in file.readlines()]\n",
    "\n",
    "fig, axes = subplots(2, 2, dpi=dpi)\n",
    "for i, dataset in enumerate(datasets):\n",
    "    features = readlines(f\"results/{dataset}.select-features.n_features-default.txt\")\n",
    "    features_scmap = readlines(f\"results/{dataset}.select-features.scmap.n_features-500.txt\")\n",
    "    ax = axes[i//2,i%2]\n",
    "    matplotlib_venn.venn2([set(features), set(features_scmap)], set_labels=(\"\", \"\"), ax=ax)\n",
    "    ax.set_title(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    fig, axes = subplots(2, 2, dpi=dpi)\n",
    "    for i, n_features in enumerate([500, 1000, 2000, 4000]):\n",
    "        features = readlines(f\"results/{dataset}.select-features.n_features-{n_features}.txt\")\n",
    "        features_scmap = readlines(f\"results/{dataset}.select-features.scmap.n_features-{n_features}.txt\")\n",
    "        ax = axes[i//2,i%2]\n",
    "        matplotlib_venn.venn2([set(features), set(features_scmap)], set_labels=(\"\", \"\"), ax=ax)\n",
    "        ax.set_title(f\"n-features = {n_features}\")\n",
    "    fig.suptitle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_features = pandas.DataFrame({\"dataset\": [], \"consistency\": [], \"cohen_kappa\": []})\n",
    "for dataset in datasets:\n",
    "    for n_features in [500, 1000, 2000, 4000]:\n",
    "        filename = f\"results/{dataset}.knn-cv-features.n_features-{n_features}.toml\"\n",
    "        out = toml.load(filename)\n",
    "        for exp in out[\"experiment\"]:\n",
    "            row = cluster_metrics(exp[\"filename\"], clusters[dataset])\n",
    "            row[\"dataset\"] = dataset\n",
    "            row[\"n-features\"] = n_features\n",
    "            knn_cv_features = knn_cv_features.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_features.groupby([\"dataset\", \"n-features\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_features_scmap = pandas.DataFrame({\"dataset\": [], \"consistency\": [], \"cohen_kappa\": []})\n",
    "for dataset in datasets:\n",
    "    for n_min_features in [\"default\", 500, 1000, 2000, 4000]:\n",
    "        filename = f\"results/{dataset}.knn-cv-features.scmap.n_features-{n_min_features}.toml\"\n",
    "        out = toml.load(filename)\n",
    "        for exp in out[\"experiment\"]:\n",
    "            row = cluster_metrics(exp[\"filename\"], clusters[dataset])\n",
    "            row[\"dataset\"] = dataset\n",
    "            row[\"n-min-features\"] = n_min_features\n",
    "            with open(exp[\"features-file\"]) as f:\n",
    "                row[\"n-features\"] = len(f.readlines())\n",
    "            knn_cv_features_scmap = knn_cv_features_scmap.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_features_scmap.groupby([\"dataset\", \"n-min-features\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster-specific scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def count_assignments(cell, nn, cluster):\n",
    "    consistent = cluster[cell].values == cluster[nn].values\n",
    "    cluster = cluster[cell]\n",
    "    # make counters\n",
    "    count_all = collections.Counter(cluster)\n",
    "    count_consistent = collections.Counter({c: 0 for c in count_all.keys()})\n",
    "    count_consistent.update(cluster[consistent])\n",
    "    count_inconsistent = collections.Counter({c: 0 for c in count_all.keys()})\n",
    "    count_inconsistent.update(cluster[~consistent])\n",
    "    # join counts\n",
    "    counts = pandas.DataFrame({\"all\": pandas.Series(count_all), \"consistent\": pandas.Series(count_consistent), \"inconsistent\": pandas.Series(count_inconsistent)})\n",
    "    counts = counts.assign(consistency=lambda x: x[\"consistent\"] / x[\"all\"])[[\"all\", \"consistent\", \"inconsistent\", \"consistency\"]]\n",
    "    counts.sort_values(by=\"all\", ascending=False, inplace=True)\n",
    "    counts.index.name = \"cluster\"\n",
    "    return counts\n",
    "\n",
    "\n",
    "def plot_assignments_matrix(cell, nn, cluster, cluster2=None, overlapping=False, figsize=(12,12)):\n",
    "    cluster1 = cluster\n",
    "    if cluster2 is None:\n",
    "        cluster2 = cluster1\n",
    "    labels1 = cluster1.value_counts().index.values\n",
    "    labels2 = cluster2.value_counts().index.values\n",
    "    if overlapping:\n",
    "        labels2 = labels1\n",
    "    assignments = pandas.crosstab(\n",
    "        pandas.Categorical(cluster1[cell].values, categories=labels1),\n",
    "        pandas.Categorical(cluster2[nn].values, categories=labels2),\n",
    "        rownames=[\"query\"],\n",
    "        colnames=[\"neighbor\"],\n",
    "        dropna=False,\n",
    "        normalize=\"index\")\n",
    "    assignments = assignments.loc[labels1,labels2]\n",
    "    fig, ax = subplots(dpi=dpi, figsize=figsize)\n",
    "    im = ax.imshow(assignments, vmin=0, vmax=1)\n",
    "    ax.set_ylabel(\"Query's label\")\n",
    "    ax.set_xlabel(\"Neighbor's label\")\n",
    "    ax.set_yticks(numpy.arange(assignments.shape[0]))\n",
    "    ax.set_xticks(numpy.arange(assignments.shape[1]))\n",
    "    ax.set_yticklabels(assignments.index)\n",
    "    ax.set_xticklabels(assignments.columns, rotation=90)\n",
    "    ax.set_yticks(numpy.arange(assignments.shape[0]+1)-.5, minor=True)\n",
    "    ax.set_xticks(numpy.arange(assignments.shape[1]+1)-.5, minor=True)\n",
    "    ax.grid(False)  # turn off seaborn's grid\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.8)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.tick_params(length=0)\n",
    "    cbar = fig.colorbar(im, fraction=0.046, pad=0.02, shrink=0.3, aspect=30)\n",
    "    cbar.outline.set_visible(False)\n",
    "    for x in [\"left\", \"right\", \"top\", \"bottom\"]:\n",
    "        ax.spines[x].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap long labels\n",
    "def wraplabel(label, width=16):\n",
    "    return \"\\n\".join(textwrap.wrap(label.replace(\"_\", \" \"), width=width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    cluster = clusters[dataset]\n",
    "    # CellFishing\n",
    "    filenames = knn_cv[(knn_cv[\"dataset\"]==dataset)&(knn_cv[\"n-bits\"]==128)&(knn_cv[\"n-lshashes\"]==4)&(knn_cv[\"transformer\"]==\"log1p\")][\"filename\"]\n",
    "    knn = [load_knn(x) for x in filenames]\n",
    "    counts1 = [count_assignments(x.index, x[\"N1\"], cluster) for x in knn]\n",
    "    fig, ax = plot_assignments_matrix(knn[0].index, knn[0][\"N1\"], cluster)\n",
    "    savefig(f\"{dataset}-selfmapping-matrix\")\n",
    "    # scmap\n",
    "    filenames = knn_cv_scmap[(knn_cv_scmap[\"dataset\"]==dataset)&(knn_cv_scmap[\"n-features\"]==500)&(knn_cv_scmap[\"n-centroids-factor\"]==1)][\"filename\"]\n",
    "    knn = [load_knn(x) for x in filenames]\n",
    "    counts2 = [count_assignments(x.index, x[\"N1\"], cluster) for x in knn]\n",
    "    fig, ax = plot_assignments_matrix(knn[0].index, knn[0][\"N1\"], cluster)\n",
    "    savefig(f\"{dataset}-selfmapping-matrix-scmap\")\n",
    "    # merge\n",
    "    x = pandas.concat([x[\"consistency\"].reset_index() for x in counts1], ignore_index=True)\n",
    "    x[\"method\"] = \"CellFishing\"\n",
    "    y = pandas.concat([x[\"consistency\"].reset_index() for x in counts2], ignore_index=True)\n",
    "    y[\"method\"] = \"scmap-cell\";\n",
    "    data = pandas.concat([x, y], ignore_index=True)\n",
    "    cluster_unique = cluster.value_counts().index.values\n",
    "    n_cols = 3\n",
    "    y_max = math.ceil(len(cluster_unique) / n_cols)\n",
    "    fig, axes = subplots(1, n_cols, dpi=dpi, figsize=(8, math.ceil(y_max / 2)))\n",
    "    for j in range(n_cols):\n",
    "        if n_cols == 1:\n",
    "            ax = axes\n",
    "        else:\n",
    "            ax = axes[j]\n",
    "        ax.set_xlim(0, 1.02)\n",
    "        ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        y = cluster_unique[j*y_max:min((j+1)*y_max, len(cluster_unique))]\n",
    "        seaborn.swarmplot(x=\"consistency\", y=\"cluster\", hue=\"method\", data=data[data[\"cluster\"].isin(y)], order=y, orient=\"h\", dodge=False, size=2.4, ax=ax)\n",
    "        ax.tick_params(labelsize=\"small\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_yticklabels([wraplabel(x.get_text()) for x in ax.get_yticklabels()])\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "        if j > 0:\n",
    "            ax.legend_.remove()\n",
    "        else:\n",
    "            ax.legend(loc=\"upper left\", fontsize=\"small\").set_title(\"\")\n",
    "    fig.tight_layout(w_pad=0.5)\n",
    "    seaborn.despine(fig=fig)\n",
    "    savefig(f\"{dataset}-selfmapping-clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities from nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(dataset, cluster, targets):\n",
    "    fig, axes = subplots(1, len(targets), dpi=dpi, figsize=(8, 4), sharey=True)\n",
    "    for i, target in enumerate(targets):\n",
    "        target_escaped = target.replace('/', ';')\n",
    "        log = toml.load(f\"results/{dataset}.knn-similarities.target-{target_escaped}.toml\")\n",
    "        similarities_with_target_cells = pandas.read_table(log[\"experiment\"][0][\"filename-similarities-with-target-cells\"], index_col=\"cell\")\n",
    "        similarities_without_target_cells = pandas.read_table(log[\"experiment\"][0][\"filename-similarities-without-target-cells\"], index_col=\"cell\")\n",
    "        cells_target = cluster[(cluster == target).values].index.values\n",
    "        #cells_nontarget = cluster[(cluster != target).values].index.values\n",
    "        # melt data\n",
    "        df1 = similarities_with_target_cells.loc[cells_target].reset_index().melt(id_vars=\"cell\", var_name=\"rank\", value_name=\"similarity\")\n",
    "        df1[\"removed\"] = False\n",
    "        df2 = similarities_without_target_cells.loc[cells_target].reset_index().melt(id_vars=\"cell\", var_name=\"rank\", value_name=\"similarity\")\n",
    "        df2[\"removed\"] = True\n",
    "        df = df1.append(df2)\n",
    "        df[\"rank\"] = [int(x[1]) for x in df[\"rank\"]]\n",
    "        df = df[df[\"rank\"]<=3]\n",
    "        # plot\n",
    "        ax = axes[i]\n",
    "        seaborn.violinplot(x=\"rank\", y=\"similarity\", hue=\"removed\", data=df, split=True, inner=\"quart\", linewidth=0.6, ax=ax)\n",
    "        #seaborn.boxplot(x=\"rank\", y=\"similarity\", hue=\"removed\", data=df, linewidth=0.8, fliersize=0.5, ax=ax)\n",
    "        ax.set_title(wraplabel(target), fontsize=\"small\")\n",
    "        ax.legend(loc=\"lower left\")\n",
    "        ax.legend_.remove()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        if i > 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "    axes[0].legend(title=\"removed\", loc=\"lower left\", fontsize=\"small\", title_fontsize=\"small\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout()\n",
    "    savefig(f\"{dataset}-selfmapping-remove\")\n",
    "\n",
    "\n",
    "def list_nearest_neighbors(dataset, cluster, targets):\n",
    "    for target in targets:\n",
    "        target_escaped = target.replace('/', ';')\n",
    "        log = toml.load(f\"results/{dataset}.knn-similarities.target-{target_escaped}.toml\")\n",
    "        #knn_with_target_cells = pandas.read_table(log[\"experiment\"][0][\"filename-knn-with-target-cells\"], index_col=\"cell\")\n",
    "        knn_without_target_cells = pandas.read_table(log[\"experiment\"][0][\"filename-knn-without-target-cells\"], index_col=\"cell\")\n",
    "        cells_target = cluster[(cluster == target).values].index.values\n",
    "        value_counts = cluster.loc[knn_without_target_cells.loc[cells_target,\"N1\"]].value_counts()\n",
    "        print(target)\n",
    "        for i, (label, n) in enumerate(value_counts.items()):\n",
    "            if i > 4:\n",
    "                break\n",
    "            print(\"    {}: {}\".format(label, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Shekhar2016\"\n",
    "targets = [\"BC2\", \"BC5D\", \"BC3A\", \"BC5B\", \"BC4\", \"BC8/9 (mixture of BC8 and BC9)\", \"AC (Amacrine cell)\"]\n",
    "plot_similarities(dataset, clusters[dataset], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nearest_neighbors(dataset, clusters[dataset], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"TabulaMuris-chromium\"\n",
    "targets = [\"basal cell\", \"alveolar macrophage\", \"fibroblast\", \"immature B cell\", \"basophil\", \"kidney cell\", \"endothelial cell of hepatic sinusoid\"]\n",
    "plot_similarities(dataset, clusters[dataset], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nearest_neighbors(dataset, clusters[dataset], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting DEGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_degs(cluster, target, k):\n",
    "    knns = pandas.read_table(f\"results/TabulaMuris-chromium.knn-degs.target-{target}/knn.k-{k}.tsv.gz\", index_col=\"cell\")\n",
    "    degs = pandas.read_table(f\"results/TabulaMuris-chromium.knn-degs.target-{target}/degs.k-{k}.tsv.gz\", index_col=[\"cell\", \"gene\"])\n",
    "    return degs.join(cluster).join(knns[\"N1\"]).join(cluster.rename_axis(\"N1\"), on=\"N1\", rsuffix=\"N1\").drop(\"N1\", axis=1)\n",
    "\n",
    "\n",
    "def top_genes(by, n=10000, upper=-4):\n",
    "    def fetch(df):\n",
    "        x = df.sort_values(by).head(n)\n",
    "        return x[x[by] <= upper].reset_index(\"cell\", drop=True)\n",
    "    return fetch\n",
    "\n",
    "\n",
    "def make_crosstable(cluster, celltypes, target, k, by):\n",
    "    degs = load_degs(cluster, target, k)\n",
    "    x = degs.groupby(\"cell\").apply(top_genes(by)).reset_index(\"gene\")\n",
    "    return pandas.crosstab(x[\"gene\"], x[\"clusterN1\"]).loc[:,celltypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_query = \"immature B cell\"\n",
    "celltypes = ['early pro-B cell', 'late pro-B cell', 'B cell',]\n",
    "cluster = clusters[\"TabulaMuris-chromium\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_degs(cluster, celltype_query, 5).groupby(\"cell\").first()[\"clusterN1\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List top DEGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for celltype in celltypes:\n",
    "    print(celltype)\n",
    "    print(\"-\"*len(celltype))\n",
    "    for k in [5, 10, 20]:\n",
    "        print(f\"k = {k}\")\n",
    "        negative = make_crosstable(cluster, celltypes, celltype_query, k, \"negative\")\n",
    "        positive = make_crosstable(cluster, celltypes, celltype_query, k, \"positive\")\n",
    "        print(\"  negative: \", end=\"\")\n",
    "        degs = negative.sort_values(celltype, ascending=False)[celltype]\n",
    "        for i in range(10):\n",
    "            if degs.iloc[i] == 0:\n",
    "                break\n",
    "            print(\"{:s} {:d}\".format(degs.index[i], degs.iloc[i]), end=\", \")\n",
    "        print()\n",
    "        print(\"  positive: \", end=\"\")\n",
    "        degs = positive.sort_values(celltype, ascending=False)[celltype]\n",
    "        for i in range(10):\n",
    "            if degs.iloc[i] == 0:\n",
    "                break\n",
    "            print(\"{:s} {:d}\".format(degs.index[i], degs.iloc[i]), end=\", \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LaTeX tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6  # the number of top DEGs\n",
    "\n",
    "def capitalize(s):\n",
    "    return s[0].upper() + s[1:]\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    negative = make_crosstable(cluster, celltypes, celltype_query, k, \"negative\")\n",
    "    positive = make_crosstable(cluster, celltypes, celltype_query, k, \"positive\")\n",
    "    print(f\"% k = {k}\")\n",
    "    print(r\"\\begin{tabular}{\" + \"lr\" * (len(celltypes) * 2) + \"}\")\n",
    "    print(r\"\\toprule\")\n",
    "\n",
    "    for j, celltype in enumerate(celltypes):\n",
    "        print(r\"\\multicolumn{4}{c}{\", capitalize(celltype), r\"}\", end=\" & \" if j < len(celltypes)-1 else r\" \\\\\")\n",
    "    print()\n",
    "    for j in range(len(celltypes)):\n",
    "        print(r\"\\cmidrule(lr){\", 4 * j + 1, \"-\", 4 * j + 4, r\"}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "    for j in range(len(celltypes)):\n",
    "        print(r\"\\multicolumn{2}{c}{Negative} & \\multicolumn{2}{c}{Positive}\", end=\" & \" if j < len(celltypes)-1 else r\" \\\\\")\n",
    "    print()\n",
    "    for j in range(len(celltypes)):\n",
    "        print(r\"\\cmidrule(lr){\", 4 * j + 1, \"-\", 4 * j + 2, r\"}\", end=\" \")\n",
    "        print(r\"\\cmidrule(lr){\", 4 * j + 3, \"-\", 4 * j + 4, r\"}\", end=\" \")\n",
    "    print()\n",
    "\n",
    "    # Gene count\n",
    "    for i in range(n):\n",
    "        for j, celltype in enumerate(celltypes):\n",
    "            df = negative.sort_values(celltype)\n",
    "            row = df.iloc[-(i+1)]\n",
    "            print(\"\\\\textit{{{:10s}}} & {:3d}\".format(row.name if row[celltype] > 0 else \"--\", row[celltype]), end=\" & \")\n",
    "            df = positive.sort_values(celltype)\n",
    "            row = df.iloc[-(i+1)]\n",
    "            print(\"\\\\textit{{{:10s}}} & {:3d}\".format(row.name if row[celltype] > 0 else \"--\", row[celltype]), end=\" & \" if j < len(celltypes)-1 else r\" \\\\\")\n",
    "        print()\n",
    "\n",
    "    # Total count\n",
    "    print(r\"\\addlinespace\")\n",
    "    for j, celltype in enumerate(celltypes):\n",
    "        print(\"Total & {:3d}\".format(negative[celltype].sum()), end=\" & \")\n",
    "        print(\"Total & {:3d}\".format(positive[celltype].sum()), end=\" & \" if j < len(celltypes)-1 else r\" \\\\\")\n",
    "    \n",
    "    print()\n",
    "    print(r\"\\bottomrule\")\n",
    "    print(r\"\\end{tabular}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping across batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_shekhar2016 = pandas.DataFrame()\n",
    "knn_batch_shekhar2016_scmap = pandas.DataFrame()\n",
    "cluster = clusters[\"Shekhar2016\"]\n",
    "for query in [\"1\", \"2\"]:\n",
    "    print(f\"query = {query}\")\n",
    "    # CellFishing\n",
    "    log = toml.load(f\"results/Shekhar2016.knn-batch.batch-{query}.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"version-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"query\"] = query\n",
    "    exp = pandas.concat(\n",
    "        [exp,\n",
    "         pandas.DataFrame([cluster_metrics(x, cluster) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_batch_shekhar2016 = knn_batch_shekhar2016.append(exp, ignore_index=True)\n",
    "    # scmap\n",
    "    log = toml.load(f\"results/Shekhar2016.knn-batch.scmap.batch-{query}.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"session-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"query\"] = query\n",
    "    exp = pandas.concat(\n",
    "        [exp,\n",
    "         pandas.DataFrame([cluster_metrics(x, cluster) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_batch_shekhar2016_scmap = knn_batch_shekhar2016_scmap.append(exp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_plass2018 = pandas.DataFrame()\n",
    "knn_batch_plass2018_scmap = pandas.DataFrame()\n",
    "cluster = clusters[\"Plass2018\"]\n",
    "for query in [\"plan1\", \"plan2\"]:\n",
    "    print(f\"query = {query}\")\n",
    "    # CellScape\n",
    "    log = toml.load(f\"results/Plass2018.knn-batch.batch-{query}.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"version-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"query\"] = query\n",
    "    exp = pandas.concat(\n",
    "        [exp,\n",
    "         pandas.DataFrame([cluster_metrics(x, cluster) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_batch_plass2018 = knn_batch_plass2018.append(exp, ignore_index=True)\n",
    "    # scmap\n",
    "    log = toml.load(f\"results/Plass2018.knn-batch.scmap.batch-{query}.toml\")\n",
    "    print(log[\"date-time\"])\n",
    "    print(log[\"session-info\"])\n",
    "    exp = pandas.DataFrame(log[\"experiment\"])\n",
    "    exp[\"query\"] = query\n",
    "    exp = pandas.concat(\n",
    "        [exp,\n",
    "         pandas.DataFrame([cluster_metrics(x, cluster) for x in exp[\"filename\"]])],\n",
    "        axis=1)\n",
    "    knn_batch_plass2018_scmap = knn_batch_plass2018_scmap.append(exp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_shekhar2016 = knn_batch_shekhar2016[[\"query\", \"n-bits\", \"n-lshashes\", \"transformer\", \"filename\", \"consistency\", \"cohen_kappa\"]]\n",
    "knn_batch_shekhar2016_scmap = knn_batch_shekhar2016_scmap[[\"query\", \"n-features\", \"n-centroids-factor\", \"filename\", \"consistency\", \"cohen_kappa\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_plass2018 = knn_batch_plass2018[[\"query\", \"n-bits\", \"n-lshashes\", \"transformer\", \"filename\", \"consistency\", \"cohen_kappa\"]]\n",
    "knn_batch_plass2018_scmap = knn_batch_plass2018_scmap[[\"query\", \"n-features\", \"n-centroids-factor\", \"filename\", \"consistency\", \"cohen_kappa\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_shekhar2016.groupby([\"query\", \"n-bits\", \"n-lshashes\", \"transformer\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_shekhar2016_scmap.groupby([\"query\", \"n-features\", \"n-centroids-factor\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_plass2018.groupby([\"query\", \"n-bits\", \"n-lshashes\", \"transformer\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_batch_plass2018_scmap.groupby([\"query\", \"n-features\", \"n-centroids-factor\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 128\n",
    "n_lshashes = 4\n",
    "transformer = \"log1p\"\n",
    "n_features = 500\n",
    "n_centroids_factor = 1\n",
    "for metric in [\"consistency\", \"cohen_kappa\"]:\n",
    "    fig, axes = subplots(2, 2, dpi=dpi, figsize=(8, 6))\n",
    "    for i, (dataset, data, data_scmap) in enumerate([\n",
    "            (\"Shekhar2016\", knn_batch_shekhar2016, knn_batch_shekhar2016_scmap),\n",
    "            (\"Plass2018\", knn_batch_plass2018, knn_batch_plass2018_scmap)]):\n",
    "        if i > 0:\n",
    "            axes[i,0].get_shared_x_axes().join(axes[0,0], axes[i,0])\n",
    "            axes[i,1].get_shared_x_axes().join(axes[0,1], axes[i,1])\n",
    "        axes[i,0].get_shared_y_axes().join(axes[i,0], axes[i,1])\n",
    "        data = data[(data[\"n-bits\"]==n_bits)&(data[\"n-lshashes\"]==n_lshashes)&(data[\"transformer\"]==transformer)]\n",
    "        mean = knn_cv[(knn_cv[\"dataset\"]==dataset)&(knn_cv[\"n-bits\"]==n_bits)&(knn_cv[\"n-lshashes\"]==n_lshashes)&(knn_cv[\"transformer\"]==transformer)][metric].mean()\n",
    "        data_scmap = data_scmap[(data_scmap[\"n-features\"]==n_features)&(data_scmap[\"n-centroids-factor\"]==n_centroids_factor)]\n",
    "        mean_scmap = knn_cv_scmap[(knn_cv_scmap[\"dataset\"]==dataset)&(knn_cv_scmap[\"n-features\"]==n_features)&(knn_cv_scmap[\"n-centroids-factor\"]==n_centroids_factor)][metric].mean()\n",
    "        ymin = min(data[metric].min(), data_scmap[metric].min())\n",
    "        ymax = max(data[metric].max(), data_scmap[metric].max())\n",
    "        ydiff = ymax - ymin\n",
    "        axes[i,0].set_ylim(ymin - ydiff * 0.02, ymax + ydiff * 0.02)  # need to set ylim before plotting\n",
    "        seaborn.swarmplot(x=\"query\", y=metric, data=data, ax=axes[i,0])\n",
    "        axes[i,0].axhline(mean, color=\"orangered\", lw=0.5)\n",
    "        seaborn.swarmplot(x=\"query\", y=metric, data=data_scmap, ax=axes[i,1])\n",
    "        axes[i,1].axhline(mean_scmap, color=\"orangered\", lw=0.5)\n",
    "        axes[i,0].set_ylabel(dataset + \"\\n\\n\" + metric_labels[metric])\n",
    "        axes[i,1].set_ylabel(\"\")\n",
    "        axes[i,1].set_yticklabels([])\n",
    "        for j in range(2):\n",
    "            axes[i,j].locator_params(axis=\"y\", nbins=4)\n",
    "            axes[i,j].grid(axis=\"y\", alpha=0.3)\n",
    "    axes[0,0].set_title(\"CellFishing\")\n",
    "    axes[0,1].set_title(\"scmap-cell\")\n",
    "    seaborn.despine(fig=fig)\n",
    "    fig.tight_layout()\n",
    "    savefig(f\"mapping-{metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = pandas.read_table(\"data/Shekhar2016.batch-1.txt\", names=[\"cell\"])[\"cell\"]\n",
    "batch2 = pandas.read_table(\"data/Shekhar2016.batch-2.txt\", names=[\"cell\"])[\"cell\"]\n",
    "cluster = clusters[\"Shekhar2016\"]\n",
    "fig, ax = subplots(dpi=dpi, figsize=(8, 8))\n",
    "seaborn.countplot(y=\"cluster\", hue=\"batch\", data=pandas.concat([\n",
    "    pandas.DataFrame(cluster[batch1]).assign(batch=1),\n",
    "    pandas.DataFrame(cluster[batch2]).assign(batch=2),\n",
    "]), order=cluster.value_counts().index.values, log=False, ax=ax)\n",
    "ax.set_xlabel(\"cluster size\")\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(f\"Shekhar2016-clusters-batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = pandas.read_table(\"data/Plass2018.batch-plan1.txt\", names=[\"cell\"])[\"cell\"]\n",
    "batch2 = pandas.read_table(\"data/Plass2018.batch-plan2.txt\", names=[\"cell\"])[\"cell\"]\n",
    "cluster = clusters[\"Plass2018\"]\n",
    "fig, ax = subplots(dpi=dpi, figsize=(8, 8))\n",
    "seaborn.countplot(y=\"cluster\", hue=\"batch\", data=pandas.concat([\n",
    "    pandas.DataFrame(cluster[batch1]).assign(batch=\"plan1\"),\n",
    "    pandas.DataFrame(cluster[batch2]).assign(batch=\"plan2\"),\n",
    "]), order=cluster.value_counts().index.values, log=False, ax=ax)\n",
    "ax.set_xlabel(\"cluster size\")\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(f\"Plass2018-clusters-batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping across species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_species = toml.load(\"results/Baron2016.knn-species.toml\")\n",
    "print(knn_species[\"date-time\"])\n",
    "print(knn_species[\"version-info\"])\n",
    "knn_species = pandas.DataFrame(knn_species[\"experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for row in knn_species.itertuples():\n",
    "    if row.query == \"mouse\":\n",
    "        query = \"Baron2016-mouse\"\n",
    "        reference = \"Baron2016-human\"\n",
    "    else:\n",
    "        query = \"Baron2016-human\"\n",
    "        reference = \"Baron2016-mouse\"\n",
    "    items.append(cluster_metrics(row.filename, clusters[query], cluster_nn=clusters[reference]))\n",
    "knn_species = knn_species.join(pandas.DataFrame(items))\n",
    "knn_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_species_scmap = toml.load(\"results/Baron2016.knn-species.scmap.toml\")\n",
    "print(knn_species_scmap[\"date-time\"])\n",
    "print(knn_species_scmap[\"session-info\"])\n",
    "knn_species_scmap = pandas.DataFrame(knn_species_scmap[\"experiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for row in knn_species_scmap.itertuples():\n",
    "    if row.query == \"mouse\":\n",
    "        query = \"Baron2016-mouse\"\n",
    "        reference = \"Baron2016-human\"\n",
    "    else:\n",
    "        query = \"Baron2016-human\"\n",
    "        reference = \"Baron2016-mouse\"\n",
    "    items.append(cluster_metrics(row.filename, clusters[query], cluster_nn=clusters[reference]))\n",
    "knn_species_scmap = knn_species_scmap.join(pandas.DataFrame(items))\n",
    "knn_species_scmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_species.groupby([\"query\", \"infer-stats\"]).describe()[[\"consistency\",\"cohen_kappa\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_species_scmap.groupby([\"query\"]).describe()[[\"consistency\",\"cohen_kappa\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping across protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters[\"TabulaMuris-chromium\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters[\"TabulaMuris-smart\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlappings = set(clusters[\"TabulaMuris-chromium\"].unique()) & set(clusters[\"TabulaMuris-smart\"].unique())\n",
    "len(overlappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = load_knn(\"results/TabulaMuris.knn-protocol/n_nns-10.1.tsv.gz\")\n",
    "plot_assignments_matrix(nns.index.values, nns[\"N1\"].values, clusters[\"TabulaMuris-smart\"], clusters[\"TabulaMuris-chromium\"], figsize=(12, 14))\n",
    "savefig(\"TabulaMuris-smart-chromium-matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_assignments_matrix(nns.index.values, nns[\"N1\"].values,\n",
    "                        clusters[\"TabulaMuris-smart\"][clusters[\"TabulaMuris-smart\"].isin(overlappings)],\n",
    "                        clusters[\"TabulaMuris-chromium\"][clusters[\"TabulaMuris-chromium\"].isin(overlappings)],\n",
    "                        overlapping=True, figsize=(12, 14))\n",
    "savefig(\"TabulaMuris-smart-chromium-matrix-overlapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = load_knn(\"results/TabulaMuris.knn-protocol.scmap/n_nns-10.1.tsv.gz\")\n",
    "plot_assignments_matrix(nns.index.values, nns[\"N1\"].values, clusters[\"TabulaMuris-smart\"], clusters[\"TabulaMuris-chromium\"], figsize=(12, 14))\n",
    "savefig(\"TabulaMuris-smart-chromium-matrix-scmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_assignments_matrix(nns.index.values, nns[\"N1\"].values,\n",
    "                        clusters[\"TabulaMuris-smart\"][clusters[\"TabulaMuris-smart\"].isin(overlappings)],\n",
    "                        clusters[\"TabulaMuris-chromium\"][clusters[\"TabulaMuris-chromium\"].isin(overlappings)],\n",
    "                        overlapping=True, figsize=(12, 14))\n",
    "savefig(\"TabulaMuris-smart-chromium-matrix-scmap-overlapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benachmarks of saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveload = pandas.DataFrame()\n",
    "for dataset in datasets:\n",
    "    out = toml.load(f\"results/{dataset}.save-load.toml\")\n",
    "    print(out[\"date-time\"])\n",
    "    print(out[\"version-info\"])\n",
    "    exp = pandas.DataFrame(out[\"experiment\"])\n",
    "    exp[\"dataset\"] = dataset\n",
    "    for x in exp.columns:\n",
    "        if x.endswith(\"-size\"):\n",
    "            exp[x] /= 1024**2  # B => MiB\n",
    "        if x.endswith(\"-time\"):\n",
    "            exp[x] *= 1000  # s => ms\n",
    "    saveload = saveload.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveload_median = saveload\\\n",
    "    .groupby([\"dataset\", \"n-bits\", \"keep-counts\"])\\\n",
    "    .median()\\\n",
    "    [[\"n-cells\", \"n-genes\",\n",
    "      \"ram-size\", \"file-size\",\n",
    "      \"save-time\", \"load-time\",]]\n",
    "saveload_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a LaTeX table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\"\"\n",
    "\\begin{tabular}{l cc rrrr}\n",
    "\\toprule\n",
    "Data set & \\#bits & Raw counts & \\multicolumn{2}{c}{Size [MiB]} & \\multicolumn{2}{c}{Time [ms]} \\\\\n",
    "\\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n",
    "         &        &            & Memory & File                    & Save & Load \\\\\"\"\")\n",
    "for i, dataset in enumerate(datasets):\n",
    "    if i == 0:\n",
    "        print(r\"\\midrule\")\n",
    "    else:\n",
    "        print(r\"\\addlinespace\")\n",
    "    print(r\"\\multirow{4}{*}{\", dataset, \"}\")\n",
    "    for n_bits in [128, 256]:\n",
    "        print(r\"  & \\multirow{2}{*}{\", n_bits, \"} \", end=\"\")\n",
    "        for keep_counts in [False, True]:\n",
    "            if keep_counts:\n",
    "                print(\"  &                        \", end=\"\")\n",
    "            print(\" &\", r\"\\texttt{+}\" if keep_counts else r\"\\texttt{-}\", end=\"\")\n",
    "            for col in [\"ram-size\", \"file-size\", \"save-time\", \"load-time\"]:\n",
    "                row = saveload_median.loc[(dataset, n_bits, keep_counts)]\n",
    "                print(\" & {:4.1f} \".format(row[col]), end=\"\")\n",
    "            print(r\"\\\\\")\n",
    "print(r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"1M_neurons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = toml.load(f\"results/{dataset}.scalability.toml\")\n",
    "print(out[\"date-time\"])\n",
    "print(out[\"version-info\"])\n",
    "scalability = pandas.DataFrame(out[\"experiment\"])\n",
    "scalability[\"ram-size\"] /= 1024**2  # byte => MiB\n",
    "scalability = pandas.concat(\n",
    "    [scalability, pandas.DataFrame([cluster_metrics(x, clusters[dataset], k=1) for x in scalability[\"filename\"]])],\n",
    "    axis=1)\n",
    "scalability = scalability.drop([\"n-cells-query\", \"n-genes\", \"n-nns\"], axis=1)  # drop constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalability.groupby([\"n-bits\", \"n-cells-database\", \"indexed\"]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(4, 2, dpi=dpi, figsize=(8, 6), sharex=True)\n",
    "for i, metric in enumerate([\"index-time\", \"query-time\", \"ram-size\", \"consistency\"]):\n",
    "    for j, n_bits in enumerate([128, 256]):\n",
    "        ax = axes[i,j]\n",
    "        for indexed in [False, True]:\n",
    "            data = scalability[(scalability[\"n-bits\"]==n_bits)&(scalability[\"indexed\"]==indexed)].groupby(\"n-cells-database\").median().reset_index()\n",
    "            ax.plot(data[\"n-cells-database\"], data[metric], \".-\", label=\"index\" if indexed else \"linear\")\n",
    "        ax.set_xscale(\"log\", basex=2)\n",
    "        if metric != \"consistency\":\n",
    "            ax.set_yscale(\"log\")\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"n-bits = {n_bits}\")\n",
    "        if i == 3:\n",
    "            ax.set_xlabel(\"database size\")\n",
    "        if j == 0:\n",
    "            if metric == \"index-time\":\n",
    "                ax.set_ylabel(\"index time [s]\")\n",
    "            elif metric == \"query-time\":\n",
    "                ax.set_ylabel(\"query time [s]\")\n",
    "            elif metric == \"ram-size\":\n",
    "                ax.set_ylabel(\"memory size [MiB]\")\n",
    "            else:\n",
    "                ax.set_ylabel(\"consistency\")\n",
    "        if i == 0 and j == 0:\n",
    "            ax.legend()\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout()\n",
    "savefig(\"scalability-metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a LaTeX table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalability_median = scalability.groupby([\"n-bits\", \"n-cells-database\", \"indexed\"]).median()\n",
    "indexed = True\n",
    "print(r\"\"\"\n",
    "\\begin{tabular}{l cc rrrrrrrr}\n",
    "\\toprule\n",
    "& \\#bits  & Index & \\multicolumn{8}{c}{Database size $N$}\\\\\n",
    "\\cmidrule{4-11}\n",
    "&         &       & $2^{13}$ (1.0) & $2^{14}$ (2.0) & $2^{15}$ (4.0) & $2^{16}$ (8.0) & $2^{17}$ (16.0) & $2^{18}$ (32.0) & $2^{19}$ (64.0) & $2^{20}$ (128.0)\\\\\n",
    "\"\"\")\n",
    "scales = numpy.arange(13, 21)\n",
    "for i, metric in enumerate([\"index-time\", \"query-time\", \"ram-size\"]):\n",
    "    if i == 0:\n",
    "        print(r\"\\midrule\")\n",
    "    else:\n",
    "        print(r\"\\addlinespace\")\n",
    "    print(r\"\\multirow{4}{*}{\", end=\"\")\n",
    "    if metric == \"index-time\":\n",
    "        print(\"Index time [s]\", end=\"\")\n",
    "    elif metric == \"query-time\":\n",
    "        print(\"Query time [s]\", end=\"\")\n",
    "    else:\n",
    "        print(\"Memory size [MiB]\", end=\"\")\n",
    "    print(r\"}\")\n",
    "    for n_bits in [128, 256]:\n",
    "        for indexed in [False, True]:\n",
    "            print(r\"  &\", n_bits, \"&\", r\"\\texttt{+}\" if indexed else r\"\\texttt{-}\", end=\"\")\n",
    "            base = scalability_median[metric].loc[n_bits,2**scales[0],indexed]\n",
    "            for s in scales:\n",
    "                n = 2**s\n",
    "                val = scalability_median[metric].loc[n_bits,n,indexed]\n",
    "                print(\" & {:5.1f} ({:.1f})\".format(val, val/base), end=\"\")\n",
    "            print(r\"\\\\\")\n",
    "print(r\"\\bottomrule\")\n",
    "print(r\"\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = 10000\n",
    "n_bits = 128\n",
    "fig, axes = subplots(2, dpi=dpi, figsize=(8, 5))\n",
    "for i, indexed in enumerate([False, True]):\n",
    "    ax = axes[i]\n",
    "    labels = []\n",
    "    y = 0\n",
    "    for s in scales:\n",
    "        n = 2**s\n",
    "        left = 0\n",
    "        bars = []\n",
    "        for j, metric in enumerate([\"preproc-time\", \"search-time\", \"rank-time\"]):\n",
    "            time = scalability_median[metric].loc[n_bits,n,indexed] / 1000 / n_cells\n",
    "            bars.append(ax.barh(y, time, left=left, color=f\"C{j}\"))\n",
    "            left += time\n",
    "        labels.append(f\"$2^{{{s}}}$\")\n",
    "        y += 1\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_ylabel(\"database size\")\n",
    "    ax.set_xlabel(\"elapsed time [s/cell]\")\n",
    "    ax.legend(bars, [\"preprocessing\", \"searching\", \"ranking\"], title=\"phase\")\n",
    "    ax.invert_yaxis()\n",
    "    if indexed:\n",
    "        ax.set_title(\"index search\")\n",
    "    else:\n",
    "        ax.set_title(\"linear search\")\n",
    "seaborn.despine(fig=fig)\n",
    "fig.tight_layout();\n",
    "savefig(\"scalability-computational-cost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
